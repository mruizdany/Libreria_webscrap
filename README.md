# SCRAP DE LIBROS
Scrapping de los datos de libros de la libreria "La casa de los libros".   

# :notebook: Urbanismo Literature Scraper

## Descripci贸n
Urbanismo Literature Scraper es una herramienta profesional de web scraping dise帽ada espec铆ficamente para recopilar y analizar informaci贸n sobre libros y revistas relacionados **con urbanismo y transporte** de las principales **librer铆as online espa帽olas**. La herramienta utiliza t茅cnicas avanzadas de scraping respetuoso y procesamiento inteligente para identificar y extraer contenido relevante.

![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)
![BeautifulSoup4](https://img.shields.io/badge/BeautifulSoup4-4.9+-green.svg)
![Requests](https://img.shields.io/badge/requests-2.25+-yellow.svg)
![Pandas](https://img.shields.io/badge/pandas-1.2+-red.svg)

##  Caracter铆sticas Principales

- **Scraping Multi-Librer铆a**: Compatible con las principales plataformas (Casa del Libro, Fnac, Amazon)
- **Filtrado Inteligente**: Sistema avanzado de palabras clave para identificar contenido relevante
- **Control de Tiempo**: Monitorizaci贸n detallada del rendimiento por chunk y operaci贸n
- **Gesti贸n de Errores**: Sistema robusto de manejo de excepciones y reintentos
- **Logging Completo**: Registro detallado de todas las operaciones y tiempos
- **Exportaci贸n Flexible**: Salida en formato CSV con resumen estad铆stico

##  Requisitos Previos

```bash
Python 3.8 o superior
pip (gestor de paquetes de Python)
```

##  Instalaci贸n

1. Clonar el repositorio:
```bash
git clone https://github.com/yourusername/urbanismo-scraper.git
cd urbanismo-scraper
```

2. Crear un entorno virtual (recomendado):
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. Instalar dependencias:
```bash
pip install -r requirements.txt
```

##  Uso

### Uso B谩sico
```python
from urbanismo_scraper import UrbanismoScraper

scraper = UrbanismoScraper()
libros = scraper.scrapear_libros(max_paginas=5)
```

### Ejemplo Completo
```python
from urbanismo_scraper import UrbanismoScraper
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)

# Inicializar scraper
scraper = UrbanismoScraper()

try:
    # Ejecutar scraping
    libros = scraper.scrapear_libros(max_paginas=5)
    
    # Guardar resultados
    scraper.guardar_resultados(libros, 'libros_urbanismo')
    
except Exception as e:
    logging.error(f"Error en la ejecuci贸n: {str(e)}")
```

##  Estructura de Datos

Los datos se exportan en formato CSV con los siguientes campos:

| Campo | Descripci贸n |
|-------|-------------|
| titulo | T铆tulo del libro |
| autor | Autor(es) del libro |
| precio | Precio actual |
| isbn | N煤mero ISBN (10 o 13 d铆gitos) |
| url | URL can贸nica del libro |
| fecha_extraccion | Timestamp de la extracci贸n |

## 锔 Configuraci贸n

El script permite configurar varios par谩metros:

```python
# config.py
CONFIGURACION = {
    'DELAY_MIN': 2,          # Delay m铆nimo entre peticiones (segundos)
    'DELAY_MAX': 5,          # Delay m谩ximo entre peticiones (segundos)
    'MAX_REINTENTOS': 3,     # N煤mero m谩ximo de reintentos por petici贸n
    'TAMANO_CHUNK': 10,      # N煤mero de libros por chunk
    'USER_AGENT': 'Mozilla/5.0 ...'  # User agent personalizado
}
```

##  Logging

El sistema genera dos tipos de logs:

1. **Log de Operaciones** (`scraping_urbanismo_YYYYMMDD.log`):
   - Tiempos de ejecuci贸n
   - Errores y excepciones
   - Informaci贸n de progreso

2. **Resumen Estad铆stico** (`libros_urbanismo_resumen.txt`):
   - Total de libros extra铆dos
   - Autores 煤nicos
   - Rango de precios
   - Fecha de extracci贸n

## 锔 Consideraciones ticas y Legales

1. **Respeto a robots.txt**:
   - Verificar y respetar las pol铆ticas de robots.txt
   - Mantener delays apropiados entre peticiones

2. **Uso Responsable**:
   - No saturar los servidores
   - Implementar l铆mites de velocidad
   - Respetar los t茅rminos de servicio de cada sitio

3. **Datos Personales**:
   - No extraer informaci贸n personal
   - Cumplir con GDPR/LOPD cuando aplique

##  Mantenimiento y Contribuci贸n

### Reportar Problemas
Utilice el sistema de issues de GitHub para reportar problemas:
- Describa el problema detalladamente
- Incluya logs relevantes
- Proporcione pasos para reproducir el error

### Contribuir
1. Fork del repositorio
2. Crear rama feature (`git checkout -b feature/NuevaCaracteristica`)
3. Commit de cambios (`git commit -am 'A帽adir nueva caracter铆stica'`)
4. Push a la rama (`git push origin feature/NuevaCaracteristica`)
5. Crear Pull Request

##  Rendimiento

El script incluye mediciones de rendimiento detalladas:
- Tiempo por petici贸n
- Tiempo por chunk de procesamiento
- Tiempo total de ejecuci贸n
- Estad铆sticas de 茅xito/fallo

##  Licencia

Este proyecto est谩 bajo la Licencia MIT. Ver el archivo `LICENSE` para m谩s detalles.

##  Autores

- Nombre del Autor - [Correo](mailto:autor@ejemplo.com)

##  Agradecimientos

- BeautifulSoup4 por su excelente biblioteca de parsing HTML
- Requests por su robusta biblioteca HTTP
- Pandas por el manejo eficiente de datos

---
锔 con わ por [Tu Nombre/Organizaci贸n]


He creado un README profesional y completo que incluye:

1. **Badges** de versiones y dependencias
2. **Documentaci贸n detallada** de instalaci贸n y uso
3. **Consideraciones 茅ticas** importantes para web scraping
4. **Estructura clara** con emojis para mejor legibilidad
5. **Ejemplos de c贸digo** pr谩cticos
6. **Secci贸n de mantenimiento** y contribuci贸n
7. **Tablas** para explicar la estructura de datos
8. **Informaci贸n sobre logging** y rendimiento

驴Necesitas que expanda alguna secci贸n o que a帽ada informaci贸n adicional?